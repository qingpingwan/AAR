{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLIP模型的处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import torch\n",
    "\n",
    "devices = \"cuda:3\"\n",
    "\n",
    "clip_model = CLIPModel.from_pretrained(\"/data4/zxf/hf/openai/clip-vit-large-patch14\").to(devices)\n",
    "processor = CLIPProcessor.from_pretrained(\"/data4/zxf/hf/openai/clip-vit-large-patch14\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class VisualProjection(nn.Module):\n",
    "    def __init__(self, visual_projection):\n",
    "        super().__init__()\n",
    "        self.visual_projection = visual_projection\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        将输入张量 x 从 (batch, len, 768) 映射到 (batch, len, 768)\n",
    "        \"\"\"\n",
    "        x = self.visual_projection(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class TextProjection(nn.Module):\n",
    "    def __init__(self, visual_projection):\n",
    "        super().__init__()\n",
    "        self.visual_projection = visual_projection\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        将输入张量 x 从 (batch, len, 768) 映射到 (batch, len, 768)\n",
    "        \"\"\"\n",
    "        x = self.visual_projection(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "\n",
    "visual_projection = clip_model.visual_projection\n",
    "Text_projection = clip_model.text_projection\n",
    "\n",
    "\n",
    "# 创建 VisualProjection 模块并进行测试\n",
    "Visual_module = VisualProjection(visual_projection).to(devices)\n",
    "# input_tensor_1 = outputs[\"vision_model_output\"][\"last_hidden_state\"]\n",
    "# Visual_output_tensor = Visual_module(input_tensor_1)\n",
    "\n",
    "\n",
    "\n",
    "Text_module = TextProjection(Text_projection).to(devices)\n",
    "# input_tensor_2 = outputs[\"text_model_output\"][\"last_hidden_state\"]\n",
    "# Text_output_tensor = Text_module(input_tensor_2)\n",
    "\n",
    "# print(Text_output_tensor.size())\n",
    "# print(Visual_output_tensor.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CLIP_pipeline(x1,x2,x3,x4):\n",
    "    tmp_inputs_text = processor(text=x1, return_tensors=\"pt\", padding=True,truncation=True, max_length=77).to(devices)\n",
    "    tmp_inputs_image = processor(images=x2, return_tensors=\"pt\").to(devices)\n",
    "    tmp_inputs_analy1= processor(text=x3, return_tensors=\"pt\", padding=True,truncation=True, max_length=77).to(devices)\n",
    "    tmp_inputs_analy2 = processor(text=x4, return_tensors=\"pt\", padding=True,truncation=True, max_length=77).to(devices)\n",
    "    \n",
    "    outputs_1 = clip_model.text_model(**tmp_inputs_text)\n",
    "    outputs_2 = clip_model.vision_model(**tmp_inputs_image)\n",
    "    outputs_tensor_1 = outputs_1[\"last_hidden_state\"]\n",
    "    Text_output_tensor = Text_module(outputs_tensor_1)\n",
    "    outputs_tensor_2 = outputs_2[\"last_hidden_state\"]\n",
    "    Visual_output_tensor = Visual_module(outputs_tensor_2)\n",
    "    outputs_3 = clip_model.text_model(**tmp_inputs_analy1)[\"last_hidden_state\"]\n",
    "    outputs_4 = clip_model.text_model(**tmp_inputs_analy2)[\"last_hidden_state\"]\n",
    "    outputs_3 = Text_module(outputs_3)\n",
    "    outputs_4 = Text_module(outputs_4)\n",
    "    \n",
    "    \n",
    "    return Text_output_tensor, Visual_output_tensor, outputs_3, outputs_4\n",
    "\n",
    "# x1 = [\"a photo of a cat\", \"a photo of a dog\"]\n",
    "# x2 = [Image.open(requests.get(\"http://images.cocodataset.org/val2017/000000039769.jpg\", stream=True).raw),\n",
    "#       Image.open(requests.get(\"http://images.cocodataset.org/val2017/000000397133.jpg\", stream=True).raw)]\n",
    "\n",
    "# x1,x2 = CLIP_pipeline(x1,x2)\n",
    "\n",
    "# print(x1.size())\n",
    "# print(x2.size())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 神经网络层的构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class multimodal_attention(nn.Module):\n",
    "    \"\"\"\n",
    "    dot-product attention mechanism\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, attention_dropout=0.5):\n",
    "        super(multimodal_attention, self).__init__()\n",
    "        self.dropout = nn.Dropout(attention_dropout)\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "\n",
    "    def forward(self, q, k, v, scale=None, attn_mask=None):\n",
    "\n",
    "        attention = torch.matmul(q, k.transpose(-2, -1))\n",
    "        # print('attention.shape:{}'.format(attention.shape))\n",
    "        if scale:\n",
    "            attention = attention * scale\n",
    "\n",
    "        if attn_mask:\n",
    "            attention = attention.masked_fill_(attn_mask, -np.inf)\n",
    "            \n",
    "        attention = self.softmax(attention)\n",
    "        # print('attention.shftmax:{}'.format(attention))\n",
    "        attention = self.dropout(attention)\n",
    "        v_result = torch.matmul(attention, v)\n",
    "        # print('attn_final.shape:{}'.format(attention.shape))\n",
    "\n",
    "        return v_result\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class CrossAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Head Cross Attention mechanism\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_dim=768, num_heads=8, dropout=0.5):\n",
    "        super(CrossAttention, self).__init__()\n",
    "\n",
    "        self.model_dim = model_dim\n",
    "        self.dim_per_head = model_dim // num_heads\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.linear_q = nn.Linear(model_dim, self.dim_per_head * num_heads, bias=False)\n",
    "        self.linear_k = nn.Linear(model_dim, self.dim_per_head * num_heads, bias=False)\n",
    "        self.linear_v = nn.Linear(model_dim, self.dim_per_head * num_heads, bias=False)\n",
    "\n",
    "        self.dot_product_attention = multimodal_attention(dropout)\n",
    "        self.linear_final = nn.Linear(model_dim, model_dim, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(model_dim)\n",
    "\n",
    "    def forward(self, query, key, value, attn_mask=None):\n",
    "        residual = query\n",
    "\n",
    "        # Linear projection\n",
    "        query = self.linear_q(query)\n",
    "        key = self.linear_k(key)\n",
    "        value = self.linear_v(value)\n",
    "\n",
    "        # Split by heads\n",
    "        batch_size = query.size(0)\n",
    "        query = query.view(batch_size, -1, self.num_heads, self.dim_per_head).transpose(1, 2)\n",
    "        key = key.view(batch_size, -1, self.num_heads, self.dim_per_head).transpose(1, 2)\n",
    "        value = value.view(batch_size, -1, self.num_heads, self.dim_per_head).transpose(1, 2)\n",
    "\n",
    "        # Scaled dot product attention\n",
    "        scale = (self.dim_per_head) ** -0.5\n",
    "        attention = self.dot_product_attention(query, key, value, scale, attn_mask)\n",
    "\n",
    "        # Concatenate heads\n",
    "        attention = attention.transpose(1, 2).contiguous().view(batch_size, -1, self.num_heads * self.dim_per_head)\n",
    "\n",
    "        # Final linear projection\n",
    "        output = self.linear_final(attention)\n",
    "\n",
    "        # Dropout\n",
    "        output = self.dropout(output)\n",
    "\n",
    "        # Add residual and norm layer\n",
    "        output = self.layer_norm(residual + output)\n",
    "\n",
    "        return output\n",
    "\n",
    "class MultiHeadCrossAttention(nn.Module):\n",
    "    def __init__(self, model_dim=768, num_heads=8, dropout=0.5):\n",
    "        super(MultiHeadCrossAttention, self).__init__()\n",
    "\n",
    "        self.model_dim = model_dim\n",
    "        self.dim_per_head = model_dim // num_heads\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.cross_attention = CrossAttention(model_dim, num_heads, dropout)\n",
    "        self.layer_norm = nn.LayerNorm(model_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x1, x2, attn_mask=None):\n",
    "        # Cross attention from x1 to x2\n",
    "        cross_attn_output_1 = self.cross_attention(x1, x2, x2, attn_mask)\n",
    "        # Cross attention from x2 to x1\n",
    "        cross_attn_output_2 = self.cross_attention(x2, x1, x1, attn_mask)\n",
    "\n",
    "        # Combine the outputs\n",
    "        output_1 = self.layer_norm(x1 + cross_attn_output_1)\n",
    "        output_2 = self.layer_norm(x2 + cross_attn_output_2)\n",
    "\n",
    "        return output_1, output_2\n",
    "\n",
    "# # Example usage\n",
    "# batch_1, len_1, dim = 2, 10, 768\n",
    "# batch_2, len_2, dim = 2, 15, 768\n",
    "\n",
    "# x1 = torch.randn(batch_1, len_1, dim)\n",
    "# x2 = torch.randn(batch_2, len_2, dim)\n",
    "\n",
    "# layer = MultiHeadCrossAttention(model_dim=768, num_heads=8, dropout=0.5)\n",
    "# output_1, output_2 = layer(x1, x2)\n",
    "\n",
    "\n",
    "# print(\"output_1 shape:\", output_1.size())  # Expected: [batch_1, len_1, 768]\n",
    "# print(\"output_2 shape:\", output_2.size())  # Expected: [batch_2, len_2, 768]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Head Self Attention mechanism\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_dim=768, num_heads=8, dropout=0.5):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "\n",
    "        self.model_dim = model_dim\n",
    "        self.dim_per_head = model_dim // num_heads\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.linear_q = nn.Linear(model_dim, self.dim_per_head * num_heads, bias=False)\n",
    "        self.linear_k = nn.Linear(model_dim, self.dim_per_head * num_heads, bias=False)\n",
    "        self.linear_v = nn.Linear(model_dim, self.dim_per_head * num_heads, bias=False)\n",
    "\n",
    "        self.dot_product_attention = multimodal_attention(dropout)\n",
    "        self.linear_final = nn.Linear(model_dim, model_dim, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(model_dim)\n",
    "\n",
    "    def forward(self, x, attn_mask=None):\n",
    "        residual = x\n",
    "\n",
    "        # Linear projection\n",
    "        query = self.linear_q(x)\n",
    "        key = self.linear_k(x)\n",
    "        value = self.linear_v(x)\n",
    "\n",
    "        # Split by heads\n",
    "        batch_size = query.size(0)\n",
    "        query = query.view(batch_size, -1, self.num_heads, self.dim_per_head).transpose(1, 2)\n",
    "        key = key.view(batch_size, -1, self.num_heads, self.dim_per_head).transpose(1, 2)\n",
    "        value = value.view(batch_size, -1, self.num_heads, self.dim_per_head).transpose(1, 2)\n",
    "\n",
    "        # Scaled dot product attention\n",
    "        scale = (self.dim_per_head) ** -0.5\n",
    "        attention = self.dot_product_attention(query, key, value, scale, attn_mask)\n",
    "\n",
    "        # Concatenate heads\n",
    "        attention = attention.transpose(1, 2).contiguous().view(batch_size, -1, self.num_heads * self.dim_per_head)\n",
    "\n",
    "        # Final linear projection\n",
    "        output = self.linear_final(attention)\n",
    "\n",
    "        # Dropout\n",
    "        output = self.dropout(output)\n",
    "\n",
    "        # Add residual and norm layer\n",
    "        output = self.layer_norm(residual + output)\n",
    "\n",
    "        return output\n",
    "\n",
    "# # Example usage\n",
    "# batch_size = 2\n",
    "# seq_len = 10\n",
    "# model_dim = 768\n",
    "\n",
    "# x = torch.randn(batch_size, seq_len, model_dim)\n",
    "\n",
    "# self_attention = MultiHeadSelfAttention(model_dim=model_dim, num_heads=8, dropout=0.5)\n",
    "# output = self_attention(x)\n",
    "\n",
    "# print(\"output shape:\", output.size())  # Expected: [batch_size, seq_len, model_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, model_dim=768, num_heads=8, dropout=0.5):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.dim_per_head = model_dim // num_heads\n",
    "        \n",
    "        self.linear_q = nn.Linear(model_dim, model_dim)\n",
    "        self.linear_k = nn.Linear(model_dim, model_dim)\n",
    "        self.linear_v = nn.Linear(model_dim, model_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.linear_out = nn.Linear(model_dim, model_dim)\n",
    "        \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.size(0)\n",
    "        \n",
    "        query = self.linear_q(query).view(batch_size, -1, self.num_heads, self.dim_per_head).transpose(1, 2)\n",
    "        key = self.linear_k(key).view(batch_size, -1, self.num_heads, self.dim_per_head).transpose(1, 2)\n",
    "        value = self.linear_v(value).view(batch_size, -1, self.num_heads, self.dim_per_head).transpose(1, 2)\n",
    "        \n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) / (self.dim_per_head ** 0.5)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        attn = self.softmax(scores)\n",
    "        attn = self.dropout(attn)\n",
    "        \n",
    "        context = torch.matmul(attn, value).transpose(1, 2).contiguous().view(batch_size, -1, self.num_heads * self.dim_per_head)\n",
    "        output = self.linear_out(context)\n",
    "        \n",
    "        return output\n",
    "\n",
    "class CoAttention(nn.Module):\n",
    "    def __init__(self, model_dim=768, num_heads=8, dropout=0.5):\n",
    "        super(CoAttention, self).__init__()\n",
    "        self.attention1 = MultiHeadAttention(model_dim, num_heads, dropout)\n",
    "        self.attention2 = MultiHeadAttention(model_dim, num_heads, dropout)\n",
    "        self.linear_out = nn.Linear(2 * model_dim, model_dim)\n",
    "        self.layer_norm = nn.LayerNorm(model_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x1, x2, mask1=None, mask2=None):\n",
    "        attn_output1 = self.attention1(x1, x2, x2, mask2)\n",
    "        attn_output2 = self.attention2(x2, x1, x1, mask1)\n",
    "        \n",
    "        combined = torch.cat([attn_output1.mean(dim=1), attn_output2.mean(dim=1)], dim=-1)\n",
    "        output = self.dropout(self.linear_out(combined))\n",
    "        output = self.layer_norm(output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Example usage\n",
    "# batch_size, len_1, len_2, dim = 2, 10, 15, 768\n",
    "\n",
    "# x1 = torch.randn(batch_size, len_1, dim)\n",
    "# x2 = torch.randn(batch_size, len_2, dim)\n",
    "\n",
    "# model = CoAttention(model_dim=dim, num_heads=8, dropout=0.5)\n",
    "# output = model(x1, x2)\n",
    "\n",
    "# print(\"output shape:\", output.size())  # Expected: [batch, 768]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalWiseFeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    Fully-connected network\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_dim=768, ffn_dim=2048, dropout=0.5):\n",
    "        super(PositionalWiseFeedForward, self).__init__()\n",
    "        self.w1 = nn.Linear(model_dim, ffn_dim)\n",
    "        self.w2 = nn.Linear(ffn_dim, model_dim)\n",
    "        \n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(model_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        x = self.w2(F.relu(self.w1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x += residual\n",
    "\n",
    "        x = self.layer_norm(x)\n",
    "        output = x\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_features, out_features, hidden_size=256, dropout=0.5):\n",
    "        super(MLP, self).__init__()\n",
    "        self.linear1 = nn.Linear(in_features, hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(hidden_size, out_features)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "\n",
    "class VLR(nn.Module):\n",
    "    def __init__(self, dim=768):\n",
    "        super(VLR, self).__init__()\n",
    "        # Self-attention for individual modalities\n",
    "        self.text_self_attention = MultiHeadSelfAttention(model_dim=dim, num_heads=8, dropout=0.5)\n",
    "        self.image_self_attention = MultiHeadSelfAttention(model_dim=dim, num_heads=8, dropout=0.5)\n",
    "        \n",
    "        # Trainable weighting parameters for fusion\n",
    "        self.alpha = nn.Parameter(torch.tensor(0.5))\n",
    "        self.beta = nn.Parameter(torch.tensor(0.5))\n",
    "        \n",
    "        # First prediction module (fusion)\n",
    "        self.first_judge = nn.Sequential(\n",
    "            nn.Linear(dim, dim//2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(dim//2, 2)\n",
    "        )\n",
    "        \n",
    "        # Cross-attention for adversarial reasoning\n",
    "        self.adversarial_cross_attention = MultiHeadCrossAttention(\n",
    "            model_dim=dim, num_heads=8, dropout=0.5\n",
    "        )\n",
    "        \n",
    "        # Second prediction module (adversarial reasoning)\n",
    "        self.second_judge = nn.Sequential(\n",
    "            nn.Linear(dim, dim//2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(dim//2, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, text_features, image_features, adversarial_arguments_1,adversarial_arguments_2):\n",
    "        # Self-attention on individual modalities\n",
    "        R1 = self.text_self_attention(text_features).mean(dim=1)\n",
    "        R2 = self.image_self_attention(image_features).mean(dim=1)\n",
    "        \n",
    "        # Fused features with trainable weights\n",
    "        G = self.alpha * R1 + self.beta * R2\n",
    "        \n",
    "        # First prediction\n",
    "        z1 = self.first_judge(G)\n",
    "        \n",
    "        # Transform adversarial arguments using text encoder\n",
    "        # (assuming this is done before calling the forward method)\n",
    "        \n",
    "        # Adversarial reasoning module\n",
    "        # Cross-attention between fused features and adversarial arguments\n",
    "        \n",
    "        adversarial_arguments = adversarial_arguments_1.mean(dim=1) + adversarial_arguments_2.mean(dim=1)\n",
    "        \n",
    "        # print(adversarial_arguments)\n",
    "        Lg,_ = self.adversarial_cross_attention(adversarial_arguments, G)\n",
    "        \n",
    "        Lg = Lg.mean(dim=1)\n",
    "        # print(adversarial_arguments)\n",
    "        # Second prediction based on adversarial reasoning\n",
    "        z2 = self.second_judge(Lg)\n",
    "        \n",
    "        # Final outputs (softmax on both predictions)\n",
    "        z1_prob = F.softmax(z1, dim=-1)\n",
    "        z2_prob = F.softmax(z2, dim=-1)\n",
    "        \n",
    "        return z1, z2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size, len_1, len_2, dim = 4, 10, 15, 768\n",
    "\n",
    "# x1 = torch.randn(batch_size, len_1, dim)\n",
    "# x2 = torch.randn(batch_size, len_2, dim)\n",
    "# x3 = torch.randn(batch_size, 20, dim)\n",
    "# x4 = torch.randn(batch_size, 25, dim)\n",
    "\n",
    "# model = VLR()\n",
    "# output,output_1 = model(x1, x2,x3,x4)\n",
    "\n",
    "# print(\"output_1 shape:\", output_1.size()) \n",
    "\n",
    "# print(output_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "def classification_metrics(predicted, labels):\n",
    "    \"\"\"\n",
    "    计算二分类任务的精度、召回率、F1-score\n",
    "    \n",
    "    参数:\n",
    "    predicted (torch.Tensor): 模型预测的输出,形状为(batch_size,)\n",
    "    labels (torch.Tensor): 数据的标签,形状为(batch_size,)\n",
    "    \"\"\"\n",
    "    \n",
    "    # 将预测输出和标签转换为 numpy 数组\n",
    "    y_pred = predicted.detach().cpu().numpy()\n",
    "    y_true = labels.detach().cpu().numpy()\n",
    "    \n",
    "    # 计算分类指标\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=None)\n",
    "    \n",
    "    # 输出结果\n",
    "    print(\"真新闻指标:\")\n",
    "    print(f\"Precision={precision[0]:.4f}, Recall={recall[0]:.4f}, F1-score={f1[0]:.4f}\")\n",
    "    \n",
    "    print(\"假新闻指标:\")\n",
    "    print(f\"Precision={precision[1]:.4f}, Recall={recall[1]:.4f}, F1-score={f1[1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, train_loader, test_loader, criterion, optimizer, device):\n",
    "        self.model = model\n",
    "        self.train_loader = train_loader\n",
    "        self.test_loader = test_loader\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.device = device\n",
    "\n",
    "\n",
    "    def train_epoch(self):\n",
    "        self.model.train()\n",
    "        running_loss = 0.0\n",
    "        for x1, x2, x3, x4, labels in self.train_loader:\n",
    "            \n",
    "            x1, x2, x3, x4= CLIP_pipeline(x1, x2, x3, x4)\n",
    "            x1, x2, x3, x4, labels = x1.to(self.device), x2.to(self.device), x3.to(self.device), x4.to(self.device),labels.to(self.device)\n",
    "            self.optimizer.zero_grad()\n",
    "            outputs_1, outputs = self.model(x1, x2, x3, x4)\n",
    "            # print(labels)\n",
    "            # print(outputs)\n",
    "            # print(outputs_1)\n",
    "\n",
    "            loss = self.criterion(outputs, labels)+self.criterion(outputs_1, labels)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            running_loss += loss.item() * x1.size(0)\n",
    "        epoch_loss = running_loss / len(self.train_loader.dataset)\n",
    "        return epoch_loss\n",
    "\n",
    "\n",
    "    def test(self):\n",
    "        self.model.eval()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        # 累积预测输出和标签\n",
    "        all_predicted = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for x1, x2, x3, x4, labels in self.test_loader:\n",
    "                \n",
    "                x1, x2, x3, x4= CLIP_pipeline(x1, x2, x3, x4)\n",
    "                x1, x2, x3, x4, labels = x1.to(self.device), x2.to(self.device), x3.to(self.device), x4.to(self.device),labels.to(self.device)\n",
    "                self.optimizer.zero_grad()\n",
    "                outputs_1, outputs = self.model(x1, x2, x3, x4)\n",
    "                \n",
    "\n",
    "                loss = self.criterion(outputs, labels)+self.criterion(outputs_1, labels)\n",
    "\n",
    "                running_loss += loss.item() * x1.size(0)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                \n",
    "                # 将预测输出和标签添加到列表中\n",
    "                all_predicted.extend(predicted.cpu())\n",
    "                all_labels.extend(labels.cpu())\n",
    "                \n",
    "        epoch_loss = running_loss / len(self.test_loader.dataset)\n",
    "        accuracy = correct / total\n",
    "        \n",
    "        # 调用 classification_metrics 函数\n",
    "        classification_metrics(torch.tensor(all_predicted), torch.tensor(all_labels))\n",
    "        \n",
    "        return epoch_loss, accuracy\n",
    "\n",
    "    def fit(self, epochs):\n",
    "        for epoch in range(epochs):\n",
    "            train_loss = self.train_epoch()\n",
    "            val_loss, val_accuracy = self.test()\n",
    "            print(f'Epoch {epoch + 1}/{epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "learning_rate = 2e-5\n",
    "num_epochs = 40\n",
    "\n",
    "\n",
    "\n",
    "# Model, loss function, optimizer\n",
    "model = VLR(dim=768).to(devices)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import VLM_MR2_en_dataloader\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = VLM_MR2_en_dataloader.load_train_MR2(batch_size)\n",
    "test_loader = VLM_MR2_en_dataloader.load_test_MR2(batch_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer\n",
    "trainer = Trainer(model, train_loader, test_loader, criterion, optimizer, device=devices)\n",
    "trainer.fit(num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zxf_001",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
